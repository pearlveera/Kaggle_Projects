{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f242b6b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-16T13:14:10.674468Z",
     "iopub.status.busy": "2025-06-16T13:14:10.674168Z",
     "iopub.status.idle": "2025-06-16T15:51:20.059138Z",
     "shell.execute_reply": "2025-06-16T15:51:20.057970Z"
    },
    "papermill": {
     "duration": 9429.390068,
     "end_time": "2025-06-16T15:51:20.060652",
     "exception": false,
     "start_time": "2025-06-16T13:14:10.670584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "\n",
      "--- Starting Level 0 Model Training ---\n",
      "===== Fold 1 =====\n",
      "Training LGBM...\n",
      "Training XGB...\n",
      "Training CatBoost...\n",
      "===== Fold 2 =====\n",
      "Training LGBM...\n",
      "Training XGB...\n",
      "Training CatBoost...\n",
      "===== Fold 3 =====\n",
      "Training LGBM...\n",
      "Training XGB...\n",
      "Training CatBoost...\n",
      "===== Fold 4 =====\n",
      "Training LGBM...\n",
      "Training XGB...\n",
      "Training CatBoost...\n",
      "===== Fold 5 =====\n",
      "Training LGBM...\n",
      "Training XGB...\n",
      "Training CatBoost...\n",
      "\n",
      "--- Training Level 1 Meta-Model ---\n",
      "Meta-Model training complete.\n",
      "\n",
      "--- Generating Final Predictions from Stacked Ensemble ---\n",
      "\n",
      "Submission file 'submission_stacking.csv' created.\n",
      "       id             Fertilizer Name\n",
      "0  750000          DAP 14-35-14 28-28\n",
      "1  750001     17-17-17 20-20 10-26-26\n",
      "2  750002     20-20 10-26-26 14-35-14\n",
      "3  750003  14-35-14 17-17-17 10-26-26\n",
      "4  750004     20-20 10-26-26 17-17-17\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Data Loading & Initial Prep ---\n",
    "print(\"Loading data...\")\n",
    "BASE_PATH = '/kaggle/input/playground-series-s5e6/'\n",
    "train_df = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "# --- 2. Stacking Architecture Setup ---\n",
    "# We will use the original train/test dataframes inside the CV loop\n",
    "# to handle Target Encoding correctly and prevent data leakage.\n",
    "\n",
    "target_encoder = LabelEncoder()\n",
    "train_df['Fertilizer Name'] = target_encoder.fit_transform(train_df['Fertilizer Name'])\n",
    "N_CLASSES = len(target_encoder.classes_)\n",
    "\n",
    "# Create placeholders for the Level 1 training data (OOF predictions)\n",
    "oof_lgbm = np.zeros((len(train_df), N_CLASSES))\n",
    "oof_xgb = np.zeros((len(train_df), N_CLASSES))\n",
    "oof_cat = np.zeros((len(train_df), N_CLASSES))\n",
    "\n",
    "# Create placeholders for the Level 1 test data\n",
    "test_lgbm = np.zeros((len(test_df), N_CLASSES))\n",
    "test_xgb = np.zeros((len(test_df), N_CLASSES))\n",
    "test_cat = np.zeros((len(test_df), N_CLASSES))\n",
    "\n",
    "\n",
    "# --- 3. Level 0 Models with Target Encoding in CV Loop ---\n",
    "print(\"\\n--- Starting Level 0 Model Training ---\")\n",
    "N_SPLITS = 5 # Using 5 folds for robustness\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Using our best 3 models for the base layer\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['Fertilizer Name'])):\n",
    "    print(f\"===== Fold {fold+1} =====\")\n",
    "    \n",
    "    # --- Create data for this fold ---\n",
    "    train_fold_df = train_df.iloc[train_idx]\n",
    "    val_fold_df = train_df.iloc[val_idx]\n",
    "    test_fold_df = test_df.copy() # Use a copy for each fold's processing\n",
    "    \n",
    "    # --- Target Encoding (the SAFE way) ---\n",
    "    # Calculate encoding on the training part of the fold ONLY\n",
    "    te_cols = ['Soil Type', 'Crop Type']\n",
    "    for col in te_cols:\n",
    "        # Create a mapping from category to the mean of the target\n",
    "        target_mean = train_fold_df.groupby(col)['Fertilizer Name'].mean()\n",
    "        # Apply the mapping to all splits for this fold\n",
    "        train_fold_df[col + '_te'] = train_fold_df[col].map(target_mean)\n",
    "        val_fold_df[col + '_te'] = val_fold_df[col].map(target_mean)\n",
    "        test_fold_df[col + '_te'] = test_fold_df[col].map(target_mean)\n",
    "        # Fill any missing values in val/test with the global mean\n",
    "        val_fold_df[col + '_te'].fillna(train_df['Fertilizer Name'].mean(), inplace=True)\n",
    "        test_fold_df[col + '_te'].fillna(train_df['Fertilizer Name'].mean(), inplace=True)\n",
    "\n",
    "    # --- Feature Engineering (as before) ---\n",
    "    def create_features(df):\n",
    "        df['N_P_Ratio'] = df['Nitrogen'] / (df['Phosphorous'] + 1e-6)\n",
    "        df['N_K_Ratio'] = df['Nitrogen'] / (df['Potassium'] + 1e-6)\n",
    "        df['P_K_Ratio'] = df['Phosphorous'] / (df['Potassium'] + 1e-6)\n",
    "        df['Total_Nutrients'] = df['Nitrogen'] + df['Phosphorous'] + df['Potassium']\n",
    "        es = 0.6108 * np.exp((17.27 * df['Temparature']) / (df['Temparature'] + 237.3))\n",
    "        ea = (df['Humidity'] / 100) * es\n",
    "        df['VPD'] = es - ea\n",
    "        return df\n",
    "\n",
    "    train_fold_df = create_features(train_fold_df)\n",
    "    val_fold_df = create_features(val_fold_df)\n",
    "    test_fold_df = create_features(test_fold_df)\n",
    "    \n",
    "    # --- Define Features & Final Prep ---\n",
    "    features = [col for col in train_fold_df.columns if col not in ['id', 'Fertilizer Name', 'Soil Type', 'Crop Type']]\n",
    "    X_train, y_train = train_fold_df[features], train_fold_df['Fertilizer Name']\n",
    "    X_val, y_val = val_fold_df[features], val_fold_df['Fertilizer Name']\n",
    "    X_test = test_fold_df[features]\n",
    "\n",
    "    # --- Train Models ---\n",
    "    LGBM_PARAMS = {'objective': 'multiclass', 'metric': 'multi_logloss', 'n_estimators': 2000, 'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'num_leaves': 31, 'verbose': -1, 'seed': 42}\n",
    "    print(\"Training LGBM...\")\n",
    "    lgbm = lgb.LGBMClassifier(**LGBM_PARAMS).fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    oof_lgbm[val_idx] = lgbm.predict_proba(X_val)\n",
    "    test_lgbm += lgbm.predict_proba(X_test) / N_SPLITS\n",
    "\n",
    "    XGB_PARAMS = {'objective': 'multi:softprob', 'eval_metric': 'mlogloss', 'n_estimators': 2000, 'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.8, 'colsample_bytree': 0.8, 'seed': 42, 'tree_method': 'hist', 'early_stopping_rounds': 100}\n",
    "    print(\"Training XGB...\")\n",
    "    xgb_m = xgb.XGBClassifier(**XGB_PARAMS).fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    oof_xgb[val_idx] = xgb_m.predict_proba(X_val)\n",
    "    test_xgb += xgb_m.predict_proba(X_test) / N_SPLITS\n",
    "\n",
    "    CAT_PARAMS = {'objective': 'MultiClass', 'eval_metric': 'MultiClass', 'iterations': 2000, 'learning_rate': 0.01, 'depth': 6, 'random_seed': 42, 'verbose': 0, 'early_stopping_rounds': 100}\n",
    "    print(\"Training CatBoost...\")\n",
    "    cat = ctb.CatBoostClassifier(**CAT_PARAMS).fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    oof_cat[val_idx] = cat.predict_proba(X_val)\n",
    "    test_cat += cat.predict_proba(X_test) / N_SPLITS\n",
    "\n",
    "# --- 4. Level 1 Meta-Model ---\n",
    "print(\"\\n--- Training Level 1 Meta-Model ---\")\n",
    "# Concatenate the OOF predictions to create the training data for the meta-model\n",
    "meta_train_features = np.concatenate((oof_lgbm, oof_xgb, oof_cat), axis=1)\n",
    "# Concatenate the test predictions to create the test data for the meta-model\n",
    "meta_test_features = np.concatenate((test_lgbm, test_xgb, test_cat), axis=1)\n",
    "\n",
    "# A simple logistic regression is a classic, stable choice for a meta-model\n",
    "meta_model = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000)\n",
    "meta_model.fit(meta_train_features, train_df['Fertilizer Name'])\n",
    "print(\"Meta-Model training complete.\")\n",
    "\n",
    "# --- 5. Final Prediction ---\n",
    "print(\"\\n--- Generating Final Predictions from Stacked Ensemble ---\")\n",
    "final_preds_proba = meta_model.predict_proba(meta_test_features)\n",
    "\n",
    "top_3_indices = np.argsort(final_preds_proba, axis=1)[:, ::-1][:, :3]\n",
    "top_3_labels = target_encoder.inverse_transform(top_3_indices.flatten()).reshape(top_3_indices.shape)\n",
    "predictions = [' '.join(preds) for preds in top_3_labels]\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'Fertilizer Name': predictions})\n",
    "submission_df.to_csv('submission_stacking.csv', index=False)\n",
    "print(\"\\nSubmission file 'submission_stacking.csv' created.\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12184666,
     "sourceId": 91717,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9435.31382,
   "end_time": "2025-06-16T15:51:21.293822",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-16T13:14:05.980002",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
